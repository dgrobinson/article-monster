# Article Monster - Claude Context Instructions

## üö® CRITICAL: Documentation-First Methodology

**BEFORE doing ANY work in this repo, Claude MUST:**

1. **üìñ Read root documentation first** - Root .md files in the repo (README.md, etc.)
2. **üîç Audit actual functionality** - Test what's really working vs what's documented
3. **üìä Reconcile discrepancies** - Note where docs are stale or missing
4. **‚úÖ Verify production state** - Check deployed environment, not just local code
5. **üìù Update docs BEFORE adding new ones** - Fix existing before creating new

**Why this matters:**
- Documentation goes stale quickly
- Planned features get documented but never implemented  
- Code exists but isn't integrated or tested
- Production environment may differ from code

**When creating new documentation:**
- First audit what exists and its accuracy
- Update/fix existing docs before adding new ones
- Always test actual functionality, don't assume from code presence
- Coordinate all .md files for consistency

## Project Overview
This is Article Monster - a cloud service that provides:
1. **Bookmarklet Service**: Sends web articles to both Kindle and Zotero with a single bookmarklet click
2. **MCP Server**: Provides Model Context Protocol access to your Zotero library for ChatGPT and Claude

## Key Technologies
- Node.js/Express backend
- Mozilla Readability.js for content extraction
- Zotero API integration
- Kindle email delivery
- EPUB generation
- MCP (Model Context Protocol) server

## Architecture
```
Browser (with auth) ‚Üí Bookmarklet extracts content ‚Üí Cloud Service ‚Üí Kindle + Zotero
```

## Important Files to Always Read for Context

When launched in this repository, Claude should read these files to understand the project:

1. **README.md** - Complete project overview, features, setup instructions
2. **package.json** - Dependencies, scripts, project metadata
3. **src/server.js** - Main Express server and API endpoints
4. **src/epubGenerator.js** - EPUB creation for Zotero attachments
5. **src/zoteroService.js** - Zotero API integration
6. **src/emailService.js** - Kindle email delivery
7. **public/bookmarklet.js** - Client-side content extraction

## Key Context Points

### CRITICAL DESIGN PRINCIPLE
**NEVER write publication-specific code!** 
- This service must remain publication-agnostic
- ALL site-specific extraction logic comes from FiveFilters configs
- We maintain feature parity with FiveFilters Full-Text RSS
- The fivefilters-config submodule provides all site-specific rules
- If a site doesn't extract properly, the fix belongs in FiveFilters configs, NOT in our code

### Security
- This is a defensive tool for personal research management
- Extracts content from authenticated sites user already has access to
- Only ADDS items to Zotero, never modifies existing ones
- Uses test collections to protect main Zotero library

### Main Workflows
- Bookmarklet: Extract article ‚Üí Send to Kindle + Zotero simultaneously
- MCP: Provide AI assistants access to Zotero library via API

### Environment Variables
- Gmail credentials for Kindle email delivery
- Zotero API credentials
- MCP API key for AI access
- Test collection keys for safety

## Development Context
- **IMPORTANT: This service is deployed on DigitalOcean App Platform**
- **Always test in production - local testing is not the standard workflow**
- **NEVER run the service locally** - development workflow is cloud-first
- Push changes to GitHub ‚Üí Auto-deploys to DigitalOcean
- No database - stateless service using external APIs
- Browser-side extraction preserves user authentication
- Local `npm run dev` exists but is not used in standard workflow

## Available CLI Tools
Claude has access to these CLI tools for managing this project:

### GitHub CLI (`gh`)
- Check workflow status: `gh workflow list`
- View workflow runs: `gh run list`
- Trigger deployments via GitHub Actions if needed
- Manage pull requests and issues

### DigitalOcean CLI (`doctl`)
- Check app status: `doctl apps list`
- Monitor deployments: `doctl apps get-deployment <app-id> <deployment-id>`
- View logs: `doctl apps logs <app-id>`
- App ID: `214fb1d0-54f7-4a28-ba39-7db566e8a8e6`
- App URL: https://seal-app-t4vff.ondigitalocean.app

## Known Issues & Solutions

### WSJ Content Extraction
- **Root Cause**: Bookmarklet fetches FiveFilters config asynchronously but doesn't wait for it
- **What Happens**:
  1. Bookmarklet checks for cached site config (not found)
  2. Fetches config from server but continues immediately (async)
  3. Falls back to simplified extraction which fails on WSJ
  4. Config gets stored for "next time" but current extraction already failed
- **The Fix**: Bookmarklet must wait for FiveFilters config before attempting extraction
- **Note**: WSJ has a full FiveFilters config at `site-configs/wsj.com.txt` that would work if used

### EPUB Table of Contents
- epub-gen library always includes a basic TOC structure
- Setting empty titles or excludeFromToc can break EPUB generation
- Current approach: Minimal TOC with single entry for the article

## Current Known Issues (August 2025)

### üî¥ High Priority
1. **EPUB Images Not Working** - Images are not appearing in generated EPUBs despite epub-gen supporting them
   - **Root Cause**: Authentication mismatch - bookmarklet extracts HTML with image URLs from authenticated sites client-side, but server-side epub-gen cannot access auth-protected images
   - **Secondary Issues**: Relative URLs not converted to absolute, hotlink protection on some sites
   - Location: `src/epubGenerator.js`, `public/bookmarklet.js`
   - Impact: EPUBs lack visual content
   - **Fix Plan**: See "EPUB Image Fix Implementation Plan" below
   - **Status**: Phase 1 completed - relative URLs now converted to absolute

2. **Kindle Filename Formatting** - ~~Article titles appear as "ALL_CAPS_WITH_UNDERSCORES" instead of clean formatting~~
   - **FIXED**: Now uses proper capitalization with hyphens (e.g., "The-Atlantic-Article-Title.epub")
   - Location: `src/kindleSender.js`, `src/epubGenerator.js`

### üü° Medium Priority  
3. **MCP Implementation Not Yet Tested** - Code exists but no real-world testing has been performed
   - Files exist: `src/mcpServer.js`, `MCP_INTEGRATION_PLAN.md`
   - Health endpoint shows "active" but this is just a status check
   - **Status**: Never tested with actual AI assistants or API key validation
   - Next: Test with actual MCP_API_KEY and AI assistant integration (ChatGPT/Claude)

### üü¢ Recently Fixed
1. **Enhanced Bibliographic Metadata Extraction** (August 2025)
   - **Improvements Made**:
     - Extended metadata selectors to include Twitter Cards, Dublin Core, Schema.org
     - Added DOM fallbacks for author extraction (class names, itemprop, rel attributes)
     - Added DOM fallbacks for date extraction (time elements, common class patterns)
     - Improved author parsing for Zotero (splits first/last names, handles multiple authors)
   - **Files Updated**: `public/bookmarklet.js`, `src/zoteroSender.js`
   - **Impact**: Better citation quality in Zotero with more complete metadata

## EPUB Image Fix Implementation Plan

### Problem Analysis
The core issue is an authentication mismatch between client-side extraction and server-side EPUB generation:
- **Client-side** (bookmarklet): Has full access to authenticated images via browser cookies
- **Server-side** (epub-gen): Cannot access auth-protected images when generating EPUB

### Phased Implementation Approach

#### Phase 1: URL Fixing (Immediate - 5 min fix)
**Goal**: Fix relative URLs to ensure public images work correctly

**Implementation**:
```javascript
// Add to bookmarklet.js after article extraction
function fixImageUrls(html) {
  const div = document.createElement('div');
  div.innerHTML = html;
  const baseUrl = window.location.origin;
  
  div.querySelectorAll('img').forEach(img => {
    // Fix relative URLs to absolute
    if (!img.src.startsWith('http')) {
      img.src = new URL(img.src, baseUrl).href;
    }
    // Also fix srcset for responsive images
    if (img.srcset) {
      img.srcset = img.srcset.split(',').map(src => {
        const [url, descriptor] = src.trim().split(' ');
        if (!url.startsWith('http')) {
          return new URL(url, baseUrl).href + (descriptor ? ' ' + descriptor : '');
        }
        return src;
      }).join(', ');
    }
  });
  
  return div.innerHTML;
}
```

**Benefits**:
- Fixes all public site images immediately
- No performance impact
- No payload size increase

#### Phase 2: Critical Image Embedding (If Phase 1 insufficient)
**Goal**: Embed important images as base64 to preserve authenticated content

**Implementation**:
```javascript
// Add to bookmarklet.js
async function embedCriticalImages(html) {
  const div = document.createElement('div');
  div.innerHTML = html;
  const images = div.querySelectorAll('img');
  
  // Only process first 3 images between 50KB-500KB
  let processedCount = 0;
  for (let img of images) {
    if (processedCount >= 3) break;
    
    try {
      const response = await fetch(img.src);
      const blob = await response.blob();
      
      // Skip tiny images (icons) and huge images (performance)
      if (blob.size < 50000 || blob.size > 500000) continue;
      
      const reader = new FileReader();
      const base64 = await new Promise((resolve) => {
        reader.onloadend = () => resolve(reader.result);
        reader.readAsDataURL(blob);
      });
      
      img.src = base64;
      processedCount++;
    } catch (e) {
      console.warn('Failed to embed image:', img.src);
    }
  }
  
  return div.innerHTML;
}
```

**Benefits**:
- Preserves key images from authenticated sites
- Limited to 3 images to maintain performance
- Smart filtering avoids tracking pixels and huge files
- Graceful fallback for remaining images

#### Phase 3: Monitoring & Optimization (Optional future enhancement)
**Goal**: Track success rates and optimize thresholds

**Potential Enhancements**:
- Add telemetry for image success/failure rates
- Adjust size thresholds based on real usage
- Consider WebP conversion for smaller sizes
- Add user preference for image quality/quantity tradeoff

### Implementation Steps

1. **Test Current State** (5 min)
   - Test with WSJ/Atlantic article to confirm images missing
   - Check browser console for image URLs in extracted content

2. **Implement Phase 1** (10 min)
   - Add `fixImageUrls` function to bookmarklet
   - Call it on extracted content before sending to server
   - Deploy and test with public sites (BBC, NPR, etc.)

3. **Evaluate Phase 1 Results** (10 min)
   - Test with various sites
   - Document which sites now work
   - Identify if auth sites still need Phase 2

4. **Implement Phase 2 if Needed** (20 min)
   - Add `embedCriticalImages` function
   - Integrate with extraction flow
   - Test payload sizes and performance

5. **Production Deployment** (5 min)
   - Push to GitHub
   - Auto-deploy to DigitalOcean
   - Test in production environment

### Success Criteria
- ‚úÖ Public site images appear in EPUBs
- ‚úÖ At least hero/primary images from auth sites preserved
- ‚úÖ No significant performance degradation
- ‚úÖ Payload size remains under 10MB for typical articles

### Alternative Approaches Considered (Not Recommended)
1. **Cookie Forwarding**: Security risk, complex implementation
2. **Server-side Proxy**: Doesn't solve auth problem
3. **Full Base64 Conversion**: Too large payloads, memory issues
4. **Separate Image Upload**: Too complex for bookmarklet

## Auto-Context Reading Instruction

**IMPORTANT**: Every time Claude is launched in this repository, automatically read the following files in this order to understand the full project context:

1. README.md
2. package.json  
3. src/server.js
4. src/epubGenerator.js
5. Current git status via `git status`

This ensures Claude always has complete context about:
- Project purpose and architecture
- Current dependencies and scripts
- API endpoints and functionality  
- EPUB generation capabilities
- Current working state

Do this automatically without being asked, then proceed with user requests.